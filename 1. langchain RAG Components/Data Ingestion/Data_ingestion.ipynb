{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x20d798663d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading a TXT file\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader=TextLoader('speech.txt')\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='The Burj Khalifa[a] (known as the Burj Dubai prior to its inauguration) is a skyscraper in Dubai, United Arab Emirates. It is the world\\'s tallest structure. With a total height of 829.8 m (2,722 ft, or just over half a mile) and a roof height (excluding antenna, but including a 242.6 m spire)[2] of 828 m (2,717 ft), the Burj Khalifa has been the tallest structure and building in the world since its topping out in 2009, surpassing Taipei 101, the previous holder of that status.[3][4]\\n\\nConstruction of the Burj Khalifa began in 2004, with the exterior completed five years later in 2009. The primary structure is reinforced concrete and some of the structural steel for the building originated from the Palace of the Republic in East Berlin, the former East German parliament.[5] The building was opened in 2010 as part of a new development called Downtown Dubai. It was designed to be the centerpiece of large-scale, mixed-use development.\\n\\nThe building is named for the former president of the United Arab Emirates (UAE), Sheikh Khalifa bin Zayed Al Nahyan.[6] The United Arab Emirates government provided Dubai with financial support as the developer, Emaar Properties, experienced financial problems during the 2007-2008 financial crisis. Then president of the United Arab Emirates, Khalifa bin Zayed, organized federal financial support. For his support, Mohammad bin Rashid, Ruler of Dubai, changed the name from \"Burj Dubai\" to \"Burj Khalifa\" during inauguration.\\n\\nThe design is derived from the Islamic architecture of the region, such as in the Great Mosque of Samarra. The Y-shaped tripartite floor geometry is designed to optimise residential and hotel space. A buttressed central core and wings are used to support the height of the building. Although this design was derived from Tower Palace III, the Burj Khalifa\\'s central core houses all vertical transportation except egress stairs within each of the wings.[7] The structure also features a cladding system which is designed to withstand Dubai\\'s hot summer temperatures. [8] It contains a total of 57 elevators and 8 escalators.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_document=loader.load()\n",
    "text_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'FACT-SHEET.pdf', 'page': 0}, page_content='FACT SHEET\\n'),\n",
       " Document(metadata={'source': 'FACT-SHEET.pdf', 'page': 1}, page_content=''),\n",
       " Document(metadata={'source': 'FACT-SHEET.pdf', 'page': 2}, page_content='FACT SHEET - BURJ KHALIFA\\nDescribed as both a ‘Vertical City’ and ‘A Living Wonder,’ Burj Khalifa, developed by Dubai-\\nbased Emaar Properties PJSC, is the world’s tallest building and described as a ‘Global Icon’ by the Council on Tall Buildings and Urban Habitat, an honour bestowed on tall structures only once in 10 \\nor 15 years. \\nRising gracefully from the desert, Burj Khalifa honours the city with its extraordinary union of art, \\nengineering and meticulous craftsmanship.\\nAt 828 metres (2,716.5 ft), the 200 plus storey Burj Khalifa has 160 habitable levels, the most of any \\nbuilding in the world. The tower was inaugurated on January 4, 2010, to coincide with the fourth anniversary of the Accession Day of His Highness Sheikh Mohammed Bin Rashid Al Maktoum as the Ruler of Dubai.\\nArguably the world’s most prestigious address, Burj Khalifa is responsible for a number of world-\\nfirsts. The tower became the world’s tallest man-made structure just 1,325 days after excavation work started in January 2004. \\nBurj Khalifa utilised a record-breaking 330,000 cubic metres of concrete; 39,000 tonnes of steel \\nreinforcement; 103,000 square metres of glass; and 15,500 square metres of embossed stainless steel. The tower took 22 million man hours to build.\\nWith a total built-up area of 5.67 million square feet, Burj Khalifa features 1.85 million square feet \\nof residential space and over 300,000 square feet of prime office space. That is in addition to the area occupied by the Armani Hotel Dubai and the Armani Residences.\\nThe tower offers luxurious recreational and leisure facilities including four swimming pools, \\nexcluding the pool in the hotel, lounges for home owners and office owners, health and wellness facilities, a public observation deck and At.mosphere, the world’s highest fine dining restaurant  on Level 122 that is scheduled to open early 2011.\\nThe Armani Hotel Dubai opened doors to the world on April 27, 2010, with a ribbon cutting \\nceremony by fashion maestro Giorgio Armani and Mohamed Alabbar, Chairman of Emaar Properties. The hotel is home to eight unique dining experiences including Armani/Privé, an upscale lounge. The hotel additionally features the world’s first in-hotel Armani/SPA and retail offerings - Armani/Dolci, an \\nelegant Italian sweets shop; Armani/Fiori for flower arrangements with a difference; and an Armani/\\nGalleria boutique for fashion accessories from the Armani Privé collection.\\nThe At the Top, Burj Khalifa observation decks on level 148, 125 and124 are a must-see attraction \\nand offer breathtaking views of the city and the surrounding emirates.\\nOver 1,000 pieces of art by prominent International and Middle Eastern artists adorn the interiors \\nof Burj Khalifa and the surrounding Emaar Boulevard. Many of the pieces have been specially commissioned by Emaar as a tribute to the spirit of global harmony. \\nWith the opening of Armani Hotel Dubai and handover of homes in Armani Residences, and \\n'),\n",
       " Document(metadata={'source': 'FACT-SHEET.pdf', 'page': 3}, page_content='owners moving into The Residence and The Corporate Suites, Burj Khalifa has now come to  \\nlife – indeed, a true living wonder. \\nIn 2011, Dubai welcomed the New Year with an unprecedented spectacle – the world’s highest \\nfireworks at Burj Khalifa. The celebration, which also marked the first anniversary of the grand inauguration of Burj Khalifa, was the centre of attention for a global audience as it marked New Year’s Eve with a spectacular laser-lights-fireworks show, attended by over 600,000 people and \\ntelecast live to over 2 billion people around the world. \\nTALLEST OF THE TALL\\nBurj Khalifa is ranked number one all three criteria for tall buildings of the Council on Tall Buildings \\nand Urban Habitat (CTBUH). The CTBUH ranks the world’s tallest buildings based on ‘Height to Architectural Top,’ ‘Height to Highest Occupied Floor’ and ‘Height to Tip.’\\nHeight to Architectural Top:\\n  •  Burj Khalifa – 828 metres (2,717 ft)\\n  •  Taipei 101, Taiwan – 508 metres (1,667 ft)\\n  •  Shanghai World Financial Centre, China – 492 metres (1,614 ft)\\n  •  Petronas Towers, Malaysia – 452 metres (1,483 ft)\\nHeight to Highest Occupied Floor:\\n  •  Burj Khalifa – 535 metres (1,918 ft)\\n  •  Shanghai World Financial Centre, China – 474 metres (1,555 ft)\\n  •  Taipei 101, Taiwan – 438 metres (1,437 ft)\\n  •  Willis Tower, Chicago – 413 metres (1,354 ft)\\nHeight to Tip:\\n  •  Burj Khalifa – 828 metres (2,716.5 ft)\\n  •  Willis Tower, Chicago – 527 metres (1,729 ft)\\n  •  Taipei 101, Taiwan – 508 metres (1,667 ft)\\n  •  Shanghai World Financial Centre, China – 494 metres (1,622 ft)\\nWORLD RECORDS\\nBurj Khalifa holds the following world records: \\n  •   Tallest building in the world – surpassing Taipei 101 in Taiwan, which at 508 metres (1,667 \\nft) had held the title of the ‘tallest building in the world’ since it opened in 2004\\n  •   Tallest man-made structure in the world – surpassing the KVLY-TV mast in North Dakota, \\nUSA, which stands at 628.8 metres (2,063 ft) \\n  •   Tallest free-standing structure in the world – breaking the 31-year-old record of CN \\nTower, which stands at 553.33 metres (1,815.5 ft) \\n  •  Largest number of storeys in the world – 200 plus with 160 habitable storeys \\n  •  Highest occupied floor in the world – Level 160'),\n",
       " Document(metadata={'source': 'FACT-SHEET.pdf', 'page': 4}, page_content='  •  Highest outdoor observation deck in the world – Level 124\\n  •   World record for vertical concrete pumping – 605 metres, beating the previous record \\nheld by Taipei 101 (448 metres or 1,470 ft)\\n  •   Tallest service elevator in the world – 504 metres (1,654 ft), more than the height of Taipei  \\n101 (448 metres) and almost one-and-a-half times that of the Empire State Building in New  \\nYork (381 metres)\\n  •  World record for the highest installation of an aluminium and glass façade\\n  •  World’s highest swimming pool on Level 76 (over 270 metres; over 885 ft) \\nMilestones\\nExcavation started     January 2004Piling started      February 2004\\nSuperstructure started    March 2005\\nLevel 50      June 2006\\nLevel 100      January 2007Level 110      March 2007\\nLevel 120      April 2007\\nLevel 130      May 2007\\nLevel 141 (world’s tallest building)   July 2007\\nLevel 150 (world’s tallest free-standing structure)  September 2007 \\nLevel 160 (world’s tallest man-made structure)  April 2008\\nCompletion of spire pipe jacking &\\nBurj Khalifa tops out    January 2009\\nCladding completed    September 2009 \\nGrand inauguration     January 4, 2010\\nFACTS AT A GLANCE\\nFinal height      828 metres (2,716.5 ft)\\nFinal number of floors    200 with 160 habitable storeys \\nBuilt-up area     5.67 million sq ftWeight of empty building    500,000 tonnes \\nTotal concrete used\\n    330,000 cubic metres \\nTotal reinforced steel used    39,000 tonnes\\nTotal glass used for façade    103,000 square metresTotal stainless steel used for cladding  15,500 square metres \\nTotal man hours     22 million Developer     Emaar Properties PJSCArchitects and Engineers     Chicago-based Skidmore, Owings \\n      and Merrill (SOM)\\nDesigner      Adrian Smith \\nMain Contractor\\n     South Korea’s Samsung Corporation\\nProject & Construction Manager    New York-based Turner International\\nPUBLIC ACCESS'),\n",
       " Document(metadata={'source': 'FACT-SHEET.pdf', 'page': 5}, page_content='The public have access to At the Top, Burj Khalifa, Armani Hotel Dubai, The Park and the amenities \\noffered at The Club. Owners of The Residences, The Corporate Suites and Armani Residences will \\nhave unique privileges including access to the many lifestyle amenities within the tower. \\nA LOOK INSIDE: THE MAIN COMPONENTS OF THE TOWER\\nTotal built-up area – 5.76 million sq ft\\n  •   The Residence (900 homes)  \\n       Levels – 19 to 37; 43 to 72; 76 to 108        Sky Lobby – Levels 43 to 44; 76 to 77\\n  •  The Corporate Suites (37 floors)  \\n       Levels – 112 to 121; 125 to 154        The Corporate Suites/Residential Lounge – Level 123\\n  •  Armani Hotel Dubai (160 rooms)\\n       Levels – concourse, ground, and Levels 1 to 8; 38 to 39 \\n  •  Armani Residences (144 suites) \\n       Levels – 9 to 16\\n  •  At the Top, Burj Khalifa SKY\\n       Level 148\\n  •  At the Top,Burj Khalifa \\n       Level 125 and 124\\n  •  Leisure and other Amenities \\n       Parking –  Levels – B1 and B2        Swimming pools (five) – Armani Hotel, The Club [Concourse], The Club rooftop, Level 43 and        \\nLevel 76\\n       Cigar Club –  Level 1\\n       Library – Level 123        Health club –  The Club – annexure\\n       At.mosphere – Level 122\\n       Armani/SPA –  Level 3\\nThe Residence\\nThe world’s most prestigious address is home to a select few. With 900 residences including studios and one, two, three and four-bedroom apartments, The Residence, Burj Khalifa are designed for the connoisseur. The homes are spread over levels 19-108 of the tower, and the first \\nresidents are currently moving in. \\nThe lobby reception area will feature World Voices, an intriguing installation by renowned \\nartist Jaume Plensa. Composed of 196 cymbals representing the 196 countries of the world, \\nit symbolises the global collaboration that made Burj Khalifa a reality.For the convenience of '),\n",
       " Document(metadata={'source': 'FACT-SHEET.pdf', 'page': 6}, page_content='homeowners, the tower is divided into sections with exclusive Sky Lobbies on Levels 43, 76 and 123. \\nThere are state-of-the-art fitness facilities including jacuzzis on Levels 43 and 76. \\nThe Sky Lobbies on 43 and 76 both have swimming pools and a recreational room that can be \\nutilised for special gatherings and receptions. \\nThe Corporate Suites \\nThe Corporate Suites are located on the highest levels of the tower. They occupy 37 floors, with \\nthe top three floors merged into a single office. The entrance lobby is at the Concourse of the tower. In addition to valet parking, express lifts take office visitors directly to a lounge lobby at \\nLevel 123.\\nArmani Hotel Dubai \\nThe world-first Armani Hotel Dubai opened doors to the world on April 27, 2010. From the room \\ndesigns to the carefully selected textiles and fabrics, to the impeccable service, every aspect of the Armani hotel experience will bear the signature of fashion legend Giorgio Armani. \\nArmani Hotel Dubai targets connoisseurs, who value excellence, understated style and elegance. \\nOffering 160 guest rooms and suites, restaurants and a spa, Armani Hotel Dubai brings to life the Stay with Armani promise, an exceptional lifestyle experience defined by the highest standards of aesthetics and service excellence. \\nArmani Residences Dubai\\nArmani Residences Dubai has been designed personally by Giorgio Armani, and is a reflection of his personal approach to elegance and style. Located on levels 9 to 16, the 144 one and two-bedroom suites, highlight Armani’s smooth, understated style combined with space and \\nfurnishings that focus on the compatibility of materials, form and lighting, in a superb, softly \\nluminous setting. In addition to the full access to services offered by the Armani Hotel Dubai such as concierge, 24/7 room service, housekeeping, library, spa, gym and swimming pool, residents of Armani Residences also have access to recreational facilities and entertainment venues within  \\nBurj Khalifa. The first residents have moved in to Armani Residences Dubai. \\n'),\n",
       " Document(metadata={'source': 'FACT-SHEET.pdf', 'page': 7}, page_content=''),\n",
       " Document(metadata={'source': 'FACT-SHEET.pdf', 'page': 8}, page_content='At the Top, Burj Khalifa \\nSoaring high at 555 metres (1,821 ft), At the Top, Burj Khalifa SKY, is a one-of-its-kind, must-\\nexperience journey for every individual to the highest man-made vantage point on earth. Situated on Level 148 of Burj Khalifa, it offers visitors unprecedented views of the city and beyond from an \\noutdoor terrace, and features a premium lounge. The deck also features a beautiful outdoor terrace \\nand holds the Guinness World Record for the highest observation deck in the world. At the Top, Burj Khalifa located on Levels 125 and 124 offering visitors majestic, sweeping vistas \\nof Dubai, and serving as an evocative and interactive journey through the history and evolution of \\nDubai and Burj Khalifa. A ten-part journey, At the Top starts at The Dubai Mall, where visitors can buy timed tickets at an elegant welcome area featuring monumental fields of LED displays reflecting the three-core design \\ninspiration of Burj Khalifa.\\n Visitors can enjoy several interactive features including ‘Dubai Then and Now’, charting the evolution of modern Dubai and reveal how Burj Khalifa dramatically alters the city skyline. \\nTravelling at a speed of 10 metres per second, the journey to Level 124 takes approximately 60 \\nseconds. In addition to the magnificent views offered of Dubai and beyond from At The Top, Burj Khalifa, visitors can also learn more about the landmarks in the region through the innovative Viewfinder. \\nThe interactive Viewfinder replaces the traditional coin-operated telescopes in other observatories,\\nand features a video camera and touch-screen to provide information supported by advanced graphics and 3D animation.\\nAt.mosphere\\nAt.mosphere, the world’s highest restaurant is envisaged as one of the finest luxury dining and lounge experiences in the world, and is only two levels below ‘At the Top, Burj Khalifa,’ the tower’s observatory deck, At.mosphere presents a unique dining concept that appeals to all epicureans – \\nfrom  fine dining connoisseurs to lounge lovers.  \\nIntegrating two choices - lounge & grill – At.mosphere can host over 210 guests and features a \\nspacious arrival lobby, a main dining floor, private dining rooms and display cooking stations. \\nExuding style and personality, the grill at At.mosphere, open for lunch from 12.30pm to 3pm and \\nfor dinner from 7pm to 11.30pm, serves prime cuts of beef, organic poultry and seafood with emphasis on fresh products and healthy, simple cooking – grilled, broiled or baked. The food choice is complemented by an extensive beverage selection. \\nThe upscale lounge, with a seating capacity of 130 and a private area for 35, is open from 12pm \\nto 2am, pushing the boundaries of conventional lounges, and serving as an ideal stopover for a mouth-watering light lunch or even an afternoon tea. At night, the mood shifts by opening to glittering views of the city.'),\n",
       " Document(metadata={'source': 'FACT-SHEET.pdf', 'page': 9}, page_content='Leisure and Retail  \\nBurj Khalifa has a variety of lifestyle amenities offering a myriad of choices for residents, guests and \\nvisitors. The recreational facilities and entertainment venues include luxurious swimming pools, \\nexclusive residents’ lounge and a lounge for The Corporate Suites owners, serviced residences, a wellness facility and healthclub. \\nMoreover, residents can shop at Downtown Deli, a gourmet and convenience store located within \\nThe Residences lobby or relax and unwind at the state-of-the-art fitness facilities and elegant meeting spaces located on the Sky Lobbies on Levels 43 and 76. \\nParking\\nFor the convenience of residents and visitors, Burj Khalifa has 3,000 car parking spaces on two basement levels.\\nThe Offices\\nA complement to The Corporate Suites is The Offices, a 12-storey annex with direct access to Burj Khalifa and The Dubai Mall. Parking spaces for The Offices will be offered at the mall and the tower for the convenience of tenants. \\nThe Club\\nThe Club is a four-storey health and recreation annex to Burj Khalifa spread over 22,000 sq ft. Although priority is given to Burj Khalifa residents, it is also open to the public. The extensive facilities include two luxurious pools – one indoor and one on the rooftop, two gymnasiums with a \\ndedicated ladies-only gym and a contemporary spa facility.\\nArmani/Pavilion\\nAn open-air venue designed to host events, the Armani/Pavilion provides spectacular views of The \\nDubai Fountain and is exclusively managed by Armani Hotel Dubai.\\nThe Park\\nBurj Khalifa stands tall amidst The Park, an 11-hectare green oasis that surrounds the foot of the \\ntower. The Park has six spectacular water features, lush green gardens and colourful flowering trees. \\nAs part of Burj Khalifa’s ‘green’ initiative, the landscaping is irrigated using a unique condensation \\ncollection system. Water condensation from the tower’s cooling equipment is recovered, providing an estimated 15 million gallons of water a year, enough to fill 20 Olympic-sized swimming pools. \\nCelebrating the theme ‘A Tower in a Park,’ the landscaping of Burj Khalifa has distinct divisions that \\nserve the tower’s hotel, residential, spa and corporate office areas. \\nAt the core of The Park’s design is a ‘water room’ located at the base of the tower defined by walls \\nof dancing jets and pools. \\nWater Features:\\nThe Park’s 11 hectares include seven, one-of-a-kind water features, designed by California-based WET, the creators of The Dubai Fountain and the Fountains of Bellagio in Las Vegas. '),\n",
       " Document(metadata={'source': 'FACT-SHEET.pdf', 'page': 10}, page_content='These include: Strata, Stricia, Mimeo, Onda and Alluvion. \\nDESIGN, CONSTRUCTION AND ENGINEERING \\nDesign\\nThe contract to design the world’s tallest tower was awarded to the global leader in creating  \\nultra-tall structures, the Chicago office of Skidmore, Owings & Merrill LLP (SOM) with Adrian Smith as consulting design partner. \\nThe architecture features a triple-lobed footprint, an abstraction of the six-petalled desert flower, \\nHymenocallis. The tower is composed of three elements arranged around a central core. \\nThe modular, Y-shaped structure, with setbacks along each of its three wings, provides an \\ninherently stable configuration for the structure and provides attractive floor plates for residential space. Twenty-six ‘helical’ levels decrease the cross section of the tower incrementally as it  spirals skyward.\\nThe central core emerges at the top and culminates in a sculpted spire. The Y-shaped floor plan \\nmaximizes views of the Arabian Gulf. \\nInteriors\\nThe interior design of Burj Khalifa’s public areas was done by the Chicago office of SOM and was led by award-winning designer Nada Andric. It features glass, timber, stainless steel and polished dark stones, \\ntogether with silver travertine flooring, Venetian stucco walls, handmade rugs and stone flooring. \\n'),\n",
       " Document(metadata={'source': 'FACT-SHEET.pdf', 'page': 11}, page_content='CONSTRUCTION \\n'),\n",
       " Document(metadata={'source': 'FACT-SHEET.pdf', 'page': 12}, page_content='Burj Khalifa is truly the product of international collaboration; over 60 consultants including  \\n30 on-site contracting companies from around the world were involved in the project.\\nAt the peak of construction, over 12,000 professionals and skilled workers from more than 100 \\ncountries were on site every day. The world’s fastest high-capacity construction hoists, with a speed of up to 2 m/sec (120 metres/min), were used to move men and materials.\\nOver 45,000 cubic metres (1.59 million cubic ft) of concrete, weighing more than 110,000 tonnes, \\nwere used to construct the concrete and steel foundations, which feature 192 piles buried more than 50 m (164 ft) deep. \\nBurj Khalifa employs a record-breaking 330,000 cubic m (11.6 million cubic ft) of concrete;  \\n39,000 m/t of reinforced steel; 103,000 sq m (1.1 million sq ft) of glass; 15,500 sq m (166,800 sq ft) of embossed stainless steel; and the tower took 22 million man hours to build. \\nThe amount of reinforced steel used at the tower, would if laid end to end extend over a quarter of the \\nway around the world. The concrete used is equivalent to a sidewalk 1,900 kilometres (1,200 miles) in length, and the weight of 100,000 elephants. The weight of the empty building is 500,000 tonnes. \\nWork on the exterior cladding of Burj Khalifa began in May 2007 and was completed in September \\n2009. The vast project involved more than 380 skilled engineers and on-site technicians. At the initial stage of installation the team progressed at the rate of about 20 to 30 panels per day, before increasing to as many as 175 panels per day.\\nThe tower accomplished a world record for the highest installation of an aluminium and glass \\nfaçade, at a height of 512 metres (1,679.8 ft). The total weight of aluminium used on Burj Khalifa is equivalent to that of five A380 aircraft, and the total length of stainless steel ‘bull nose’ fins is 293 times the height of the Eiffel Tower in Paris. \\nBurj Khalifa’s high performance exterior cladding system is designed to withstand the UAE’s \\nsummer temperatures. In all, the 24,348 cladding panels cover a curtain wall area of 132,190 sq m, and Burj Khalifa’s shimmering exterior minimises heat transmission and saves energy. \\nIn November 2007, the highest reinforced concrete core-walls were pumped using 80 megapascals \\n(MPa) concrete from ground level. The concrete was pumped to a height of 605 metres  (1,971.8 ft), breaking the previous pumping record held by Taipei 101. The concrete pressure during pumping to this level was nearly 200 bars.\\nSTRUCTURAL ELEMENTS\\nBurj Khalifa pushes the frontiers of engineering, construction and design expertise. Its structure employs the latest advances in wind engineering, structural engineering, structural systems, construction materials and construction methods. '),\n",
       " Document(metadata={'source': 'FACT-SHEET.pdf', 'page': 13}, page_content='Foundation\\nThe superstructure is supported by a large reinforced concrete mat, which is in turn supported by \\nbored reinforced concrete piles. The mat is 3.7 metres thick, and was constructed in four separate pours totalling 12,500 cubic metres of concrete. \\nPodium\\nThe podium provides a base anchoring the tower to the ground, allowing access from three different sides to three different levels of the building. Fully glazed entry pavilions built with a suspended cable-net structure provide separate entries for the Corporate Suites at B1 and \\nConcourse Levels, the Burj Khalifa residences at Ground Level and the Armani Hotel at Level 1.\\nStructural Core\\nIn addition to its aesthetic and functional advantages, the spiraling “Y” shaped plan was utilised to \\nshape the structural core of Burj Khalifa. This design helps to reduce the wind forces on the tower, as well as to keep the structure simple and enhance ‘constructability’. The structural system can be \\ndescribed as a “buttressed core”, and consists of high-performance concrete wall construction. \\nWind Engineering\\nThe shape of the Burj Khalifa is the result of innovative research by SOM’s architects and engineers \\nto vary the shape of the building along its height, thereby minimising wind forces on the building. Each uniquely-shaped section of the tower causes the wind to behave differently; preventing it \\nfrom becoming ‘organised’ and therefore minimising lateral movement of the structure. \\nHigh-performance, reinforced concrete core walls are linked to the exterior columns through a \\nseries of reinforced concrete shear wall panels at the mechanical levels.  Extensive wind tunnel \\ntesting ensured that the tower design is able to resist high wind loads while minimising vibration.  \\nAs well as wind tunnel studies, the team performed a detailed climatic study which considered the unique meteorological conditions of Dubai. These studies evaluated both frequently occurring and rare wind events to address occupant comfort and building strength.  \\nSpire\\nThe crowning feature of Burj Khalifa is its ‘telescopic’ spire comprising more than 4,000 tonnes of structural steel. It can be seen from 95 km (60 miles) away. The spire was built inside the building and jacked to its full height of over 200 metres (700 feet) using hydraulic strand jacks. The spire \\nis integral to the overall design, creating a sense of completion for the landmark. The spire also \\nhouses communications equipment.\\nMechanical Floors\\nSeven double-storey mechanical floors house the equipment that bring Burj Khalifa to life. Located every 30 storeys, the mechanical floors house the electrical sub-stations, water tanks and pumps,  \\nair-handling units etc, that are essential for the operation of the tower and the comfort of its occupants.\\nBroadcast and Communications Floors\\nThe top four floors have been reserved for communications and broadcasting. These floors occupy the levels just below the spire.'),\n",
       " Document(metadata={'source': 'FACT-SHEET.pdf', 'page': 14}, page_content='Elevators & Lifts\\nBurj Khalifa features 57 elevators and eight escalators to meet the travel needs of the residents and \\nvisitors to the tower. The building service/fireman’s elevator has a capacity of 5,500 kg and is the world’s tallest service elevator.\\nBurj Khalifa is the first ‘mega-high rise’ building in which certain elevators are programmed to permit \\ncontrolled evacuation for certain fire or security events. Burj Khalifa’s observatory elevators are double deck cabs with a capacity for 12 to 14 people per cab. They travel at 10 metres per second.\\nThe total number of stairs to Level 160 of Burj Khalifa is 2,909; to climb even higher, ladders are used. \\nMAINTENANCE\\nMechanical, Electrical & Plumbing\\nTo achieve the greatest efficiencies, the mechanical, electrical and plumbing services for  \\nBurj Khalifa were developed all together during the design phase of the tower in cooperation with the architect, structural engineers and other consultants.\\nThe tower’s water system supplies an average of 946,000 litres (250,000 gallons) of water daily.  \\nAt peak cooling, Burj Khalifa will require about 10,000 tonnes of cooling, equal to the cooling capacity provided by about 10,000 tonnes of melting ice. \\nDubai’s hot, humid climate combined with the building’s cooling requirements creates a significant \\namount of condensation. This water is collected and drained in a separate piping system to a holding tank in the basement car park. The condensate collection system provides about 15 million gallons of supplement water per year, equal to about 20 Olympic-sized swimming pools. The tower’s peak \\nelectrical demand is 36mW, equal to about 360,000 bulbs of 100 watts operating simultaneously.\\nWindow Washing Bays\\nAccess to the tower’s exterior for both window washing and façade maintenance is provided by \\n18 permanently installed track and fixed-telescopic, cradle-equipped building maintenance units. The track-mounted units are stored in garages, within the structure, and are not visible when not \\nin use. The manned cradles are capable of accessing the entire facade from the top of the tower \\ndown to level seven. Window washing is done vertically from the top to the ground and it takes approximately four months to clean the whole exterior. \\nFire Safety & Security \\nFire safety and speed of evacuation were prime consideration in the design of Burj Khalifa. Concrete surrounds all stairwells and the building service and fireman’s elevator has a capacity of 5,500 kg and is the world’s tallest service elevator. There are pressurized, air-conditioned refuge \\nareas located approximately every 25 floors.'),\n",
       " Document(metadata={'source': 'FACT-SHEET.pdf', 'page': 15}, page_content='1 Mohammed Bin Rashid Boulevard, Downtown Dubai\\nT: 800 At the Top  l  867 843 28 800\\ninfo@atthetop.ae   www.atthetop.ae')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading a PDF file\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader=PyPDFLoader('FACT-SHEET.pdf')\n",
    "pdf_doc=loader.load()\n",
    "pdf_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web based loader\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "loader=WebBaseLoader(web_paths=(\"https://medium.com/@sunidhi.ashtekar/yolov10-revolutionizing-real-time-object-detection-72ef04ad441a\",),\n",
    "                     bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                         class_=(\"post-title\",\"post-content\",\"post-header\")\n",
    "                     )))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://medium.com/@sunidhi.ashtekar/yolov10-revolutionizing-real-time-object-detection-72ef04ad441a'}, page_content='')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2\\nFigure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\noutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 · 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3 · 1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12\\nAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\n')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# arxiv\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "loader=ArxivLoader(query=\"1706.03762\",load_max_docs=2).load()\n",
    "# len(loader)/\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Burj Khalifa', 'summary': 'The Burj Khalifa (known as the Burj Dubai prior to its inauguration) is a skyscraper in Dubai, United Arab Emirates. It is the world\\'s tallest structure. With a total height of 829.8 m (2,722 ft, or just over half a mile) and a roof height (excluding antenna, but including a 242.6 m spire) of 828 m (2,717 ft), the Burj Khalifa has been the tallest structure and building in the world since its topping out in 2009, surpassing Taipei 101, the previous holder of that status.\\nConstruction of the Burj Khalifa began in 2004, with the exterior completed five years later in 2009. The primary structure is reinforced concrete and some of the structural steel for the building originated from the Palace of the Republic in East Berlin, the former East German parliament. The building was opened in 2010 as part of a new development called Downtown Dubai. It was designed to be the centerpiece of large-scale, mixed-use development. \\nThe building is named for the former president of the United Arab Emirates (UAE), Sheikh Khalifa bin Zayed Al Nahyan. The United Arab Emirates government provided Dubai with financial support as the developer, Emaar Properties, experienced financial problems during the 2007-2008 financial crisis. Then president of the United Arab Emirates, Khalifa bin Zayed, organized federal financial support. For his support, Mohammad bin Rashid, Ruler of Dubai, changed the name from \"Burj Dubai\" to \"Burj Khalifa\" during inauguration.\\nThe design is derived from the Islamic architecture of the region, such as in the Great Mosque of Samarra. The Y-shaped tripartite floor geometry is designed to optimise residential and hotel space. A buttressed central core and wings are used to support the height of the building. Although this design was derived from Tower Palace III, the Burj Khalifa\\'s central core houses all vertical transportation except egress stairs within each of the wings. The structure also features a cladding system which is designed to withstand Dubai\\'s hot summer temperatures.  It contains a total of 57 elevators and 8 escalators.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Burj_Khalifa'}, page_content='The Burj Khalifa (known as the Burj Dubai prior to its inauguration) is a skyscraper in Dubai, United Arab Emirates. It is the world\\'s tallest structure. With a total height of 829.8 m (2,722 ft, or just over half a mile) and a roof height (excluding antenna, but including a 242.6 m spire) of 828 m (2,717 ft), the Burj Khalifa has been the tallest structure and building in the world since its topping out in 2009, surpassing Taipei 101, the previous holder of that status.\\nConstruction of the Burj Khalifa began in 2004, with the exterior completed five years later in 2009. The primary structure is reinforced concrete and some of the structural steel for the building originated from the Palace of the Republic in East Berlin, the former East German parliament. The building was opened in 2010 as part of a new development called Downtown Dubai. It was designed to be the centerpiece of large-scale, mixed-use development. \\nThe building is named for the former president of the United Arab Emirates (UAE), Sheikh Khalifa bin Zayed Al Nahyan. The United Arab Emirates government provided Dubai with financial support as the developer, Emaar Properties, experienced financial problems during the 2007-2008 financial crisis. Then president of the United Arab Emirates, Khalifa bin Zayed, organized federal financial support. For his support, Mohammad bin Rashid, Ruler of Dubai, changed the name from \"Burj Dubai\" to \"Burj Khalifa\" during inauguration.\\nThe design is derived from the Islamic architecture of the region, such as in the Great Mosque of Samarra. The Y-shaped tripartite floor geometry is designed to optimise residential and hotel space. A buttressed central core and wings are used to support the height of the building. Although this design was derived from Tower Palace III, the Burj Khalifa\\'s central core houses all vertical transportation except egress stairs within each of the wings. The structure also features a cladding system which is designed to withstand Dubai\\'s hot summer temperatures.  It contains a total of 57 elevators and 8 escalators.\\n\\n\\n== Development ==\\nConstruction began on 12 January 2004, with the exterior of the structure completed on 1 October 2009. The building officially opened on 4 January 2010 and is part of the 2 km2 (490-acre) Downtown Dubai development at the \\'First Interchange\\' along Sheikh Zayed Road, near Dubai\\'s main business district. \\nThe tower\\'s architecture and engineering were performed by Skidmore, Owings & Merrill of Chicago, with Adrian Smith as chief architect, and Bill Baker as a chief structural engineer. The firm had designed the Sears Tower in Chicago, a previous record holder for the world\\'s tallest building.\\nHyder Consulting was supervising engineer and NORR Group Consultants supervised the architecture The primary contractor was Samsung C&T of South Korea, together with the Belgian group BESIX and the local company Arabtec. \\nNumerous complaints concerned migrant workers from South Asia, the primary building labour force, who were paid low wages and sometimes had their passports confiscated.\\n\\n\\n== Conception ==\\nBurj Khalifa was designed to be the centerpiece of a large-scale, mixed-use development to include 30,000 homes, nine hotels (including The Address Downtown Dubai), 3 hectares (7.4 acres) of parkland, at least 19 residential skyscrapers, the Dubai Mall, and the 12-hectare (30-acre) artificial Burj Khalifa Lake. The decision to build Burj Khalifa was reportedly based on the government\\'s decision to diversify from an oil-based economy to one that is service and tourism based. According to officials, projects like Burj Khalifa needed to be built to garner more international recognition and hence investment. \"He (Sheikh Mohammed bin Rashid Al Maktoum) wanted to put Dubai on the map with something really sensational,\" said Jacqui Josephson, a tourism and VIP delegations executive at Nakheel Properties. \\nThe tower was known as Burj Dubai (\"Dubai Tower\") until its official opening in January 20'),\n",
       " Document(metadata={'title': 'At the Top (Burj Khalifa)', 'summary': 'At the Top Burj Khalifa is an outdoor observation deck on the Burj Khalifa. It opened on 5 January 2010 on the 124th floor. At 452 m (1,483 ft), it was the highest outdoor observation deck in the world when it opened. \\nAlthough it was surpassed in December 2011 by Cloud Top 488 on the Canton Tower, Guangzhou at 488 m (1,601 ft), Burj Khalifa opened the 148th floor SKY level at 500 m (1,600 ft), once again giving it the highest observation deck in the world on 15 October 2014, until the Shanghai Tower opened in 2016 with an observation deck at a height of 562 metres. The 124th floor observation deck also features a so-called electronic telescope, an augmented reality device developed by Gsmprjct° of Montréal, which allows visitors to view the surrounding landscape in real-time, and to view previously saved images such as those taken at different times of day or under different weather conditions. To reduce the daily rush of sightseers, management allows visitors to purchase tickets in advance for a specific date and time, at a 75% discount on tickets purchased on the spot.\\nOn 8 February 2010, the observation deck was closed to the public for two months after power-supply problems caused an elevator to become stuck between floors, trapping a group of tourists for 45 minutes.\\nWhen the tide is low and visibility is high, people can see the shores of Iran from the top of the skyscraper.\\n\\nThe Burj Khalifa also opened another observation deck relating to At the Top, on the 148th floor, called the SKY level. This allowed the Burj Khalifa to reclaim the title of the highest observation deck in the world, surpassing the Canton Tower in Guangzhou, although its record was later surpassed by the Shanghai Tower.\\nIt still holds the record for the highest outdoor terrace in the world, and the highest restaurant in the world.', 'source': 'https://en.wikipedia.org/wiki/At_the_Top_(Burj_Khalifa)'}, page_content='At the Top Burj Khalifa is an outdoor observation deck on the Burj Khalifa. It opened on 5 January 2010 on the 124th floor. At 452 m (1,483 ft), it was the highest outdoor observation deck in the world when it opened. \\nAlthough it was surpassed in December 2011 by Cloud Top 488 on the Canton Tower, Guangzhou at 488 m (1,601 ft), Burj Khalifa opened the 148th floor SKY level at 500 m (1,600 ft), once again giving it the highest observation deck in the world on 15 October 2014, until the Shanghai Tower opened in 2016 with an observation deck at a height of 562 metres. The 124th floor observation deck also features a so-called electronic telescope, an augmented reality device developed by Gsmprjct° of Montréal, which allows visitors to view the surrounding landscape in real-time, and to view previously saved images such as those taken at different times of day or under different weather conditions. To reduce the daily rush of sightseers, management allows visitors to purchase tickets in advance for a specific date and time, at a 75% discount on tickets purchased on the spot.\\nOn 8 February 2010, the observation deck was closed to the public for two months after power-supply problems caused an elevator to become stuck between floors, trapping a group of tourists for 45 minutes.\\nWhen the tide is low and visibility is high, people can see the shores of Iran from the top of the skyscraper.\\n\\nThe Burj Khalifa also opened another observation deck relating to At the Top, on the 148th floor, called the SKY level. This allowed the Burj Khalifa to reclaim the title of the highest observation deck in the world, surpassing the Canton Tower in Guangzhou, although its record was later surpassed by the Shanghai Tower.\\nIt still holds the record for the highest outdoor terrace in the world, and the highest restaurant in the world.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "loader=WikipediaLoader(query=\"Burj-Khalifa\",load_max_docs=2).load()\n",
    "\n",
    "loader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
